{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anna Stefania Laino\\OneDrive\\Documenti\\github\\non_linearity_code\\data\\\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'baseline.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 175\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame()  \u001b[38;5;66;03m# Return an empty DataFrame if no data\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# Process each folder\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m result_baseline \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompliance_limits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m results_1_3 \u001b[38;5;241m=\u001b[39m process_folder(folder_path_2, compliance_limits)\n\u001b[0;32m    177\u001b[0m results_1_5 \u001b[38;5;241m=\u001b[39m process_folder(folder_path_3, compliance_limits)\n",
      "Cell \u001b[1;32mIn[11], line 159\u001b[0m, in \u001b[0;36mprocess_folder\u001b[1;34m(folder_path, compliance_limits)\u001b[0m\n\u001b[0;32m    157\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data_df)\n\u001b[0;32m    158\u001b[0m csv_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(folder_path)\n\u001b[1;32m--> 159\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcsv_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# Concat data file to data_folder\u001b[39;00m\n\u001b[0;32m    162\u001b[0m data_folder\u001b[38;5;241m.\u001b[39mappend(data_file)\n",
      "File \u001b[1;32mc:\\Users\\Anna Stefania Laino\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:3772\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3761\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3763\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3764\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3765\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3769\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3770\u001b[0m )\n\u001b[1;32m-> 3772\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3775\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3777\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3789\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Anna Stefania Laino\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py:1186\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1165\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1167\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1168\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1169\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1184\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1185\u001b[0m )\n\u001b[1;32m-> 1186\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1189\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\Anna Stefania Laino\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:240\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    250\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    251\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    256\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    257\u001b[0m     )\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\Anna Stefania Laino\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'baseline.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# CONSTANTS\n",
    "BOD_UPPER = 50\n",
    "BOD_LOWER = 25\n",
    "COD_UPPER = 250\n",
    "COD_LOWER = 125\n",
    "BOD_PC = 0.7\n",
    "COD_PC = 0.75\n",
    "\n",
    "# Define folder paths\n",
    "root_path = os.getcwd() + '\\data\\\\'\n",
    "print(root_path)\n",
    "\n",
    "# sub folders\n",
    "folder_path_1 = root_path + 'baseline'\n",
    "folder_path_2 = root_path + '1.3'\n",
    "folder_path_3 = root_path + '1.5'\n",
    "folder_path_4 = root_path + '1.9'\n",
    "\n",
    "compliance_limits = {\n",
    "    \"bod_upper\": BOD_UPPER,\n",
    "    \"bod_lower\": BOD_LOWER,\n",
    "    \"cod_upper\": COD_UPPER,\n",
    "    \"cod_lower\": COD_LOWER,\n",
    "    \"bod_pc\" : BOD_PC,\n",
    "    \"cod_pc\" : COD_PC\n",
    "}\n",
    "\n",
    "def process_folder(folder_path, compliance_limits):\n",
    "    data_df = []  # Data for output CSV\n",
    "    data_folder = []  # Accumulate data for results\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Skip non-Pickle files\n",
    "        if not filename.endswith(\".pkl\"):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        data_file = []\n",
    "        \n",
    "        try:\n",
    "            data_file = pd.read_pickle(file_path)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Warning: Empty DataFrame in file {file_path}\")\n",
    "            continue\n",
    "\n",
    "        if data_file.empty or any(col not in data_file.columns for col in ['bod1', 'cod1', 'bod31', 'cod31', 'snh1', 'snh31']):\n",
    "            print(f\"Warning: Missing columns in file {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        # BOD limit calculations\n",
    "        data_file['BODut'] = abs((compliance_limits[\"bod_upper\"] - data_file[\"bod31\"].min()) / (data_file[\"bod1\"].max() - data_file[\"bod31\"].min()))\n",
    "        data_file['BODlt'] = abs((compliance_limits[\"bod_lower\"] - data_file[\"bod31\"].min()) / (data_file[\"bod1\"].max() - data_file[\"bod31\"].min()))\n",
    "\n",
    "        # COD limit calculations\n",
    "        data_file['CODut'] = abs((compliance_limits[\"cod_upper\"] - data_file[\"cod31\"].min()) / (data_file[\"cod1\"].max() - data_file[\"cod31\"].min()))\n",
    "        data_file['CODlt'] = abs((compliance_limits[\"cod_lower\"] - data_file[\"cod31\"].min()) / (data_file[\"cod1\"].max() - data_file[\"cod31\"].min()))\n",
    "\n",
    "        # Lineration of BOD influent and effluent\n",
    "        data_file[\"LIN_BODe\"] = abs((data_file[\"bod31\"] - data_file[\"bod31\"].min()) / (data_file[\"bod1\"].max() - data_file[\"bod31\"].min()))\n",
    "        data_file[\"LIN_BODi\"] = abs((data_file[\"bod1\"] - data_file[\"bod31\"].min()) / (data_file[\"bod1\"].max() - data_file[\"bod31\"].min()))\n",
    "\n",
    "        # Lineration of COD influent and effluent\n",
    "        data_file[\"LIN_CODe\"] = abs((data_file[\"cod31\"] - data_file[\"cod31\"].min()) / (data_file[\"cod1\"].max() - data_file[\"cod31\"].min()))\n",
    "        data_file[\"LIN_CODi\"] = abs((data_file[\"cod1\"] - data_file[\"cod31\"].min()) / (data_file[\"cod1\"].max() - data_file[\"cod31\"].min()))\n",
    "\n",
    "        # pass \n",
    "        data_file['BODut-BODeffl'] = (data_file[\"BODut\"] - data_file[\"LIN_BODe\"])\n",
    "        data_file['CODut-CODeffl'] = (data_file[\"CODut\"] - data_file[\"LIN_CODe\"])\n",
    "        data_file['BODlt-BODeffl'] = (data_file[\"BODlt\"] - data_file[\"LIN_BODe\"])\n",
    "        data_file['CODlt-CODeffl'] = (data_file[\"CODlt\"] - data_file[\"LIN_CODe\"])\n",
    "\n",
    "        data_file[\"bodp\"] = -(compliance_limits[\"bod_pc\"] * data_file[\"LIN_BODi\"]) + data_file[\"LIN_BODi\"] - data_file[\"LIN_BODe\"]\n",
    "        data_file[\"codp\"] = -(compliance_limits[\"cod_pc\"] * data_file[\"LIN_CODi\"]) + data_file[\"LIN_CODi\"] - data_file[\"LIN_CODe\"]\n",
    "\n",
    "        # compliance requirements\n",
    "        # BOD\n",
    "        data_file[\"bod_psi_1_min\"] = data_file[['BODlt-BODeffl', 'bodp']].min(axis=1)\n",
    "        data_file[\"bod_psi_1\"] = data_file[['BODlt-BODeffl', 'bod_psi_1_min']].max(axis=1)\n",
    "        data_file[\"bod_psi_2\"] = data_file[['BODlt-BODeffl', 'bodp', \"BODut-BODeffl\"]].min(axis=1)\n",
    "        data_file[\"bod_psi_3\"] = data_file[[\"BODut-BODeffl\", \"bodp\"]].min(axis=1)\n",
    "\n",
    "        # COD\n",
    "        data_file[\"cod_psi_1_min\"] = data_file[['CODlt-CODeffl', 'codp']].min(axis=1)\n",
    "        data_file[\"cod_psi_1\"] = data_file[['CODlt-CODeffl', 'cod_psi_1_min']].max(axis=1)\n",
    "        data_file[\"cod_psi_2\"] = data_file[['CODlt-CODeffl', 'codp', \"CODut-CODeffl\"]].min(axis=1)\n",
    "        data_file[\"cod_psi_3\"] = data_file[[\"CODut-CODeffl\", \"codp\"]].min(axis=1)\n",
    "\n",
    "        #metric LUT\n",
    "        data_file[\"metric_lut\"] = data_file[[\"bod_psi_2\", \"cod_psi_2\"]].min(axis=1) # metric 1\n",
    "        #metric MAX\n",
    "        data_file[\"metric_max\"] = data_file[[\"bod_psi_3\", \"cod_psi_3\"]].min(axis=1) # metric 2\n",
    "\n",
    "        # Flags\n",
    "        data_file[\"flag_BODlt\"] = data_file['BODlt-BODeffl'] >= 0\n",
    "        data_file[\"flag_reduction_bod_compliant\"] = data_file[\"bodp\"] >= 0\n",
    "        data_file[\"flag_BODut\"] = data_file['BODut-BODeffl'] >= 0\n",
    "        data_file[\"flag_BODlt\"] = data_file['BODlt-BODeffl'] >= 0\n",
    "        data_file[\"flag_CODlt\"] = data_file['CODlt-CODeffl'] >= 0\n",
    "        data_file[\"flag_reduction_bod\"] = data_file[\"bodp\"] >= 0\n",
    "        data_file[\"flag_reduction_cod\"] = data_file[\"codp\"] >= 0\n",
    "        data_file[\"flag_BODut\"] = data_file['BODut-BODeffl'] >= 0\n",
    "        data_file[\"flag_CODut\"] = data_file['CODut-CODeffl'] >= 0\n",
    "\n",
    "        # BOD FAILURE conditions\n",
    "        bod_lut_exc = (data_file['bod_psi_2'] < 0) & (data_file[\"flag_BODlt\"] == False) & (data_file[\"flag_reduction_bod\"] == True) & (data_file[\"flag_BODut\"] == False)\n",
    "        bod_max_lim = (data_file['bod_psi_3'] < 0) & (data_file[\"flag_BODut\"] == False) & (data_file[\"flag_reduction_bod\"] == False) \n",
    "\n",
    "        # COD FAILURE conditions\n",
    "        cod_lut_exc = (data_file['cod_psi_2'] < 0) & (data_file[\"flag_CODlt\"] == False) & (data_file[\"flag_reduction_cod\"] == True) & (data_file[\"flag_CODut\"] == False)\n",
    "        cod_max_lim = (data_file['cod_psi_3'] < 0) & (data_file[\"flag_CODut\"] == False) & (data_file[\"flag_reduction_cod\"] == False) \n",
    "\n",
    "        condition_2 = (bod_lut_exc | cod_lut_exc) \n",
    "        condition_3 = (bod_max_lim | cod_max_lim)\n",
    "\n",
    "        # metric - Vectorized fix\n",
    "        cond_lut = (condition_2) & (~condition_3)\n",
    "        cond_max = (condition_2) & (condition_3)\n",
    "        \n",
    "        choices = [data_file[\"metric_lut\"], data_file[\"metric_max\"]]\n",
    "        default_val = data_file[['bod_psi_1', 'cod_psi_1']].max(axis=1)\n",
    "        \n",
    "        data_file[\"metric\"] = np.select([cond_lut, cond_max], choices, default=default_val)\n",
    "\n",
    "        # Apply the conditions and set fail_type\n",
    "        fail_type_conditions = [\n",
    "            bod_lut_exc, bod_max_lim,  # BOD conditions\n",
    "            cod_lut_exc, cod_max_lim   # COD conditions\n",
    "        ]\n",
    "\n",
    "        fail_type_values = [\n",
    "            \"LUT exceedance\", \"Max Limit Failure\",  # BOD fail types\n",
    "            \"LUT exceedance\", \"Max Limit Failure\"   # COD fail types\n",
    "        ]\n",
    "\n",
    "        # Assign the fail_type using np.select()\n",
    "        data_file[\"fail_type\"] = np.select(fail_type_conditions, fail_type_values, default=\"Compliant\")\n",
    "\n",
    "        # Set the fail_source based on which condition was met\n",
    "        data_file[\"fail_source\"] = np.select(\n",
    "            [bod_lut_exc | bod_max_lim, cod_lut_exc | cod_max_lim], \n",
    "            [\"BOD\", \"COD\"], \n",
    "            default=\"None\"\n",
    "        )\n",
    "\n",
    "        # Optionally, for \"LUT exceedance\" or \"Max Limit Failure\" conditions, set additional flags\n",
    "        data_file[\"bod_lut_exc\"] = bod_lut_exc\n",
    "        data_file[\"bod_max_lim\"] = bod_max_lim\n",
    "        data_file[\"cod_lut_exc\"] = cod_lut_exc\n",
    "        data_file[\"cod_max_lim\"] = cod_max_lim\n",
    "\n",
    "        df = pd.DataFrame(data_df)\n",
    "        csv_name = os.path.basename(folder_path)\n",
    "        df.to_csv(f'{csv_name}.csv', index=False) \n",
    "        \n",
    "        # Concat data file to data_folder\n",
    "        data_folder.append(data_file)\n",
    "        \n",
    "    # Return the concatenated results from all the files in the folder\n",
    "    if not data_folder == []:\n",
    "        # Create a DataFrame from the accumulated data\n",
    "        result_df =  pd.concat(data_folder, ignore_index=True)\n",
    "        result_df.to_csv(f\"{csv_name}_full.csv\", index=False)\n",
    "        return result_df\n",
    "    else:\n",
    "        print(f\"Warning: No data to return for folder {folder_path}\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if no data\n",
    "\n",
    "# Process each folder\n",
    "result_baseline = process_folder(folder_path_1, compliance_limits)\n",
    "results_1_3 = process_folder(folder_path_2, compliance_limits)\n",
    "results_1_5 = process_folder(folder_path_3, compliance_limits)\n",
    "results_1_9 = process_folder(folder_path_4, compliance_limits)\n",
    "\n",
    "\n",
    "datasets = [\n",
    "    (result_baseline, \"Baseline\"),\n",
    "    (results_1_3, \"Shift 130%\"),\n",
    "    (results_1_5, \"Shift 150%\"),\n",
    "    (results_1_9, \"Shift 190%\")\n",
    "]\n",
    "\n",
    "title = [\"Baseline\", \"Shift 130%\", \"Shift 150%\", \"Shift 190%\"]\n",
    "\n",
    "# Define the x and y columns for each case\n",
    "xy_columns = [\n",
    "    (\"LIN_BODi\", \"c_BOD\"),\n",
    "    (\"LIN_BODe\", \"c_BOD\"),\n",
    "    (\"LIN_CODi\", \"c_COD\"),\n",
    "    (\"LIN_CODe\", \"c_COD\")\n",
    "]\n",
    "\n",
    "# Count where fail_type is \"Compliant\", \"LUT exceedance\", \"Max Limit Failure\" and fail_source counts\n",
    "baseline_compliant_count = (result_baseline[\"fail_type\"] == \"Compliant\").sum()\n",
    "baseline_lut_count = (result_baseline[\"fail_type\"] == \"LUT exceedance\").sum()\n",
    "baseline_max_lim = (result_baseline[\"fail_type\"] == \"Max Limit Failure\").sum()\n",
    "baseline_bod_source = (result_baseline[\"fail_source\"] == \"BOD\").sum()\n",
    "baseline_cod_source = (result_baseline[\"fail_source\"] == \"COD\").sum()\n",
    "baseline_pass_source = (result_baseline[\"fail_source\"] == \"None\").sum()\n",
    "\n",
    "_1_3_compliant_count = (results_1_3[\"fail_type\"] == \"Compliant\").sum()\n",
    "_1_3_lut_count = (results_1_3[\"fail_type\"] == \"LUT exceedance\").sum()\n",
    "_1_3_max_lim = (results_1_3[\"fail_type\"] == \"Max Limit Failure\").sum()\n",
    "_1_3_bod_source = (results_1_3[\"fail_source\"] == \"BOD\").sum()\n",
    "_1_3_cod_source = (results_1_3[\"fail_source\"] == \"COD\").sum()\n",
    "_1_3_pass_source = (results_1_3[\"fail_source\"] == \"None\").sum()\n",
    "\n",
    "_1_5_compliant_count = (results_1_5[\"fail_type\"] == \"Compliant\").sum()\n",
    "_1_5_lut_count = (results_1_5[\"fail_type\"] == \"LUT exceedance\").sum()\n",
    "_1_5_max_lim = (results_1_5[\"fail_type\"] == \"Max Limit Failure\").sum()\n",
    "_1_5_bod_source = (results_1_5[\"fail_source\"] == \"BOD\").sum()\n",
    "_1_5_cod_source = (results_1_5[\"fail_source\"] == \"COD\").sum()\n",
    "_1_5_pass_source = (results_1_5[\"fail_source\"] == \"None\").sum()\n",
    "\n",
    "_1_9_compliant_count = (results_1_9[\"fail_type\"] == \"Compliant\").sum()\n",
    "_1_9_lut_count = (results_1_9[\"fail_type\"] == \"LUT exceedance\").sum()\n",
    "_1_9_max_lim = (results_1_9[\"fail_type\"] == \"Max Limit Failure\").sum()\n",
    "_1_9_bod_source = (results_1_9[\"fail_source\"] == \"BOD\").sum()\n",
    "_1_9_cod_source = (results_1_9[\"fail_source\"] == \"COD\").sum()\n",
    "_1_9_pass_source = (results_1_9[\"fail_source\"] == \"None\").sum()\n",
    "\n",
    "# Create a DataFrame to display the results in a table format\n",
    "data = {\n",
    "    \"Category\": [\"Baseline\", \"130%\", \"150%\", \"190%\"],\n",
    "    \"Compliant Count\": [baseline_compliant_count, _1_3_compliant_count, _1_5_compliant_count, _1_9_compliant_count],\n",
    "    \"LUT Exceedance Count\": [baseline_lut_count, _1_3_lut_count, _1_5_lut_count, _1_9_lut_count],\n",
    "    \"Max Limit Failure Count\": [baseline_max_lim, _1_3_max_lim, _1_5_max_lim, _1_9_max_lim],\n",
    "    \"BOD Source Count\": [baseline_bod_source, _1_3_bod_source, _1_5_bod_source, _1_9_bod_source],\n",
    "    \"COD Source Count\": [baseline_cod_source, _1_3_cod_source, _1_5_cod_source, _1_9_cod_source],\n",
    "    \"Pass Source Count\": [baseline_pass_source, _1_3_pass_source, _1_5_pass_source, _1_9_pass_source]\n",
    "}\n",
    "\n",
    "# Convert the dictionary into a pandas DataFrame\n",
    "results_df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame as a table\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple numeric non-stationarity measure\n",
    "def non_stationarity_score(series, window=30):\n",
    "    \"\"\"\n",
    "    Returns a numeric score indicating non-stationarity.\n",
    "    Higher score → more non-stationary.\n",
    "    \"\"\"\n",
    "    rolling_var = series.rolling(window=window).var()\n",
    "    overall_var = series.var()\n",
    "    \n",
    "    # Measure: mean absolute deviation of rolling variance normalized by overall variance\n",
    "    score = (rolling_var - overall_var).abs().mean() / overall_var\n",
    "    return score\n",
    "\n",
    "# Prepare a table of non-stationarity scores\n",
    "series_to_check = [\"LIN_BODi\", \"LIN_BODe\", \"LIN_CODi\", \"LIN_CODe\"]\n",
    "\n",
    "results_ns = []\n",
    "\n",
    "for df, label in datasets:\n",
    "    if df.empty:\n",
    "        continue\n",
    "    row = {\"Dataset\": label}\n",
    "    for col in series_to_check:\n",
    "        row[col] = non_stationarity_score(df[col])\n",
    "    results_ns.append(row)\n",
    "\n",
    "ns_df = pd.DataFrame(results_ns)\n",
    "display(ns_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a new figure for each plot\n",
    "fig, axs = plt.subplots(2, 4, figsize=(18, 12))  # 3 rows, 4 columns\n",
    "\n",
    "# Define the titles and data for each scenario and parameter\n",
    "parameters = ['BOD', 'COD']\n",
    "scenarios = ['Baseline', 'Scenario 130% shift', 'Scenario 150% shift', 'Scenario 190% shift']\n",
    "\n",
    "param_to_influent = {\n",
    "    'BOD': [result_baseline[\"bod1\"], results_1_3[\"bod1\"], results_1_5[\"bod1\"], results_1_9[\"bod1\"]],\n",
    "    'COD': [result_baseline[\"cod1\"], results_1_3[\"cod1\"], results_1_5[\"cod1\"], results_1_9[\"cod1\"]],\n",
    "}\n",
    "\n",
    "param_to_effluent = {\n",
    "    'BOD': [result_baseline[\"bod31\"], results_1_3[\"bod31\"], results_1_5[\"bod31\"], results_1_9[\"bod31\"]],\n",
    "    'COD': [result_baseline[\"cod31\"], results_1_3[\"cod31\"], results_1_5[\"cod31\"], results_1_9[\"cod31\"]],\n",
    "}\n",
    "\n",
    "# Loop through parameters and scenarios to generate the plots\n",
    "for i, param in enumerate(parameters):\n",
    "    for j, scenario in enumerate(scenarios):\n",
    "        # Sort the influent and effluent data\n",
    "        influent_sorted = np.sort(param_to_influent[param][j])\n",
    "        # Sort the corresponding effluent data based on the sorted influent data\n",
    "        effluent_sorted = np.array(param_to_effluent[param][j])[np.argsort(param_to_influent[param][j])]\n",
    "        \n",
    "        # Plot the sorted data\n",
    "        axs[i, j].scatter(influent_sorted, effluent_sorted, label=f'{scenario} {param}')\n",
    "        axs[i, j].set_xlabel('Influent')\n",
    "        axs[i, j].set_ylabel('Effluent')\n",
    "        axs[i, j].set_title(f'{param} Influent vs Effluent ({scenario})')\n",
    "        axs[i, j].legend()\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example arrays for each scenario (replace these with your actual concatenated arrays)\n",
    "# Assuming the lists are already concatenated as described before\n",
    "\n",
    "# Create DataFrames and calculate statistics\n",
    "stats_baseline = pd.DataFrame({\n",
    "    'mean': [result_baseline[\"bod1\"].mean(), result_baseline[\"bod31\"].mean(), result_baseline[\"cod1\"].mean(), result_baseline[\"cod31\"].mean()],\n",
    "    'std':  [result_baseline[\"bod1\"].std(),  result_baseline[\"bod31\"].std(),  result_baseline[\"cod1\"].std(),  result_baseline[\"cod31\"].std()],\n",
    "    'min':  [result_baseline[\"bod1\"].min(),  result_baseline[\"bod31\"].min(),  result_baseline[\"cod1\"].min(),  result_baseline[\"cod31\"].min()],\n",
    "    'max':  [result_baseline[\"bod1\"].max(),  result_baseline[\"bod31\"].max(),  result_baseline[\"cod1\"].max(),  result_baseline[\"cod31\"].max()]\n",
    "}, index=['BOD Influent', 'BOD Effluent', 'COD Influent', 'COD Effluent'])\n",
    "\n",
    "# Repeat for other scenarios\n",
    "stats_shift130 = pd.DataFrame({\n",
    "    'mean': [results_1_3[\"bod1\"].mean(), results_1_3[\"bod31\"].mean(), results_1_3[\"cod1\"].mean(), results_1_3[\"cod31\"].mean()],\n",
    "    'std':  [results_1_3[\"bod1\"].std(),  results_1_3[\"bod31\"].std(),  results_1_3[\"cod1\"].std(),  results_1_3[\"cod31\"].std()],\n",
    "    'min':  [results_1_3[\"bod1\"].min(),  results_1_3[\"bod31\"].min(),  results_1_3[\"cod1\"].min(),  results_1_3[\"cod31\"].min()],\n",
    "    'max':  [results_1_3[\"bod1\"].max(),  results_1_3[\"bod31\"].max(),  results_1_3[\"cod1\"].max(),  results_1_3[\"cod31\"].max()]\n",
    "}, index=['BOD Influent', 'BOD Effluent', 'COD Influent', 'COD Effluent'])\n",
    "\n",
    "stats_shift150 = pd.DataFrame({\n",
    "    'mean': [results_1_5[\"bod1\"].mean(), results_1_5[\"bod31\"].mean(), results_1_5[\"cod1\"].mean(), results_1_5[\"cod31\"].mean()],\n",
    "    'std':  [results_1_5[\"bod1\"].std(),  results_1_5[\"bod31\"].std(),  results_1_5[\"cod1\"].std(),  results_1_5[\"cod31\"].std()],\n",
    "    'min':  [results_1_5[\"bod1\"].min(),  results_1_5[\"bod31\"].min(),  results_1_5[\"cod1\"].min(),  results_1_5[\"cod31\"].min()],\n",
    "    'max':  [results_1_5[\"bod1\"].max(),  results_1_5[\"bod31\"].max(),  results_1_5[\"cod1\"].max(),  results_1_5[\"cod31\"].max()]\n",
    "}, index=['BOD Influent', 'BOD Effluent', 'COD Influent', 'COD Effluent'])\n",
    "\n",
    "stats_shift190 = pd.DataFrame({\n",
    "    'mean': [results_1_9[\"bod1\"].mean(), results_1_9[\"bod31\"].mean(), results_1_9[\"cod1\"].mean(), results_1_9[\"cod31\"].mean()],\n",
    "    'std':  [results_1_9[\"bod1\"].std(),  results_1_9[\"bod31\"].std(),  results_1_9[\"cod1\"].std(),  results_1_9[\"cod31\"].std()],\n",
    "    'min':  [results_1_9[\"bod1\"].min(),  results_1_9[\"bod31\"].min(),  results_1_9[\"cod1\"].min(),  results_1_9[\"cod31\"].min()],\n",
    "    'max':  [results_1_9[\"bod1\"].max(),  results_1_9[\"bod31\"].max(),  results_1_9[\"cod1\"].max(),  results_1_9[\"cod31\"].max()]\n",
    "}, index=['BOD Influent', 'BOD Effluent', 'COD Influent', 'COD Effluent'])\n",
    "\n",
    "# Combine all the statistics into one DataFrame\n",
    "combined_stats = pd.concat([stats_baseline, stats_shift130, stats_shift150, stats_shift190], axis=1)\n",
    "combined_stats.columns = pd.MultiIndex.from_product([['Baseline', '130% Shift', '150% Shift', '190% Shift'], ['mean', 'std', 'min', 'max']])\n",
    "\n",
    "# Plot the statistics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Statistical summary across scenarios')\n",
    "\n",
    "combined_stats.xs('mean', level=1, axis=1).plot(kind='bar', ax=axes[0, 0], title='Mean')\n",
    "combined_stats.xs('std', level=1, axis=1).plot(kind='bar', ax=axes[0, 1], title='Standard Deviation')\n",
    "combined_stats.xs('min', level=1, axis=1).plot(kind='bar', ax=axes[1, 0], title='Minimum')\n",
    "combined_stats.xs('max', level=1, axis=1).plot(kind='bar', ax=axes[1, 1], title='Maximum')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coefficient of Variation (CV)\n",
    "cv = combined_stats.xs('std', level=1, axis=1) / combined_stats.xs('mean', level=1, axis=1)\n",
    "\n",
    "# Plot the Coefficient of Variation\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "cv.plot(kind='bar', ax=ax, title='Coefficient of Variation across scenarios')\n",
    "ax.set_ylabel('CV (SD/Mean)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recovery_time(series, time_step=0.08):\n",
    "    non_compliance_duration = 0\n",
    "    recovery_times = []\n",
    "    in_non_compliance = False\n",
    "\n",
    "    for failure_type in series:\n",
    "        if failure_type in ['Max Limit Failure', 'LUT exceedance']:\n",
    "            if not in_non_compliance:\n",
    "                in_non_compliance = True\n",
    "            non_compliance_duration += 1\n",
    "        else:\n",
    "            if in_non_compliance:\n",
    "                recovery_time = round(non_compliance_duration * time_step, 2)\n",
    "                recovery_times.append(recovery_time)\n",
    "                non_compliance_duration = 0\n",
    "                in_non_compliance = False\n",
    "\n",
    "    # Check for ongoing non-compliance at the end of the series\n",
    "    if in_non_compliance:\n",
    "        recovery_time = round(non_compliance_duration * time_step, 2)\n",
    "        recovery_times.append(recovery_time)\n",
    "\n",
    "    return recovery_times\n",
    "\n",
    "# Compute recovery times for each series\n",
    "recovery_times_baseline = compute_recovery_time(result_baseline[\"fail_type\"])\n",
    "recovery_times_shift130 = compute_recovery_time(results_1_3[\"fail_type\"])\n",
    "recovery_times_shift150 = compute_recovery_time(results_1_5[\"fail_type\"])\n",
    "recovery_times_shift190 = compute_recovery_time(results_1_9[\"fail_type\"])\n",
    "\n",
    "\n",
    "# Define the bins for the histogram\n",
    "bins = 20\n",
    "time_step = 0.08\n",
    "hours_day = 24\n",
    "minutes_hour = 60\n",
    "\n",
    "# Convert recovery times to minutes\n",
    "recovery_times_baseline_minutes = [time * time_step * hours_day * minutes_hour for time in recovery_times_baseline]\n",
    "recovery_times_shift130_minutes = [time * time_step * hours_day * minutes_hour for time in recovery_times_shift130]\n",
    "recovery_times_shift150_minutes = [time * time_step * hours_day * minutes_hour for time in recovery_times_shift150]\n",
    "recovery_times_shift190_minutes = [time * time_step * hours_day * minutes_hour for time in recovery_times_shift190]\n",
    "\n",
    "# Plot histograms for each scenario\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.hist(recovery_times_baseline_minutes, bins=bins, alpha=0.5, label='Baseline')\n",
    "plt.hist(recovery_times_shift130_minutes, bins=bins, alpha=0.5, label='Shift130')\n",
    "plt.hist(recovery_times_shift150_minutes, bins=bins, alpha=0.5, label='Shift150')\n",
    "plt.hist(recovery_times_shift190_minutes, bins=bins, alpha=0.5, label='Shift190')\n",
    "\n",
    "plt.xlabel('Recovery Time (Minutes)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Recovery Times')\n",
    "plt.legend()\n",
    "plt.ylim(0,10)\n",
    "plt.show()\n",
    "# Convert lists to NumPy arrays\n",
    "recovery_times_baseline_minutes = np.array(recovery_times_baseline_minutes)\n",
    "recovery_times_shift130_minutes = np.array(recovery_times_shift130_minutes)\n",
    "recovery_times_shift150_minutes = np.array(recovery_times_shift150_minutes)\n",
    "recovery_times_shift190_minutes = np.array(recovery_times_shift190_minutes)\n",
    "\n",
    "# Calculate the mean for each dataset\n",
    "mean_baseline = np.mean(recovery_times_baseline_minutes)\n",
    "mean_shift130 = np.mean(recovery_times_shift130_minutes)\n",
    "mean_shift150 = np.mean(recovery_times_shift150_minutes)\n",
    "mean_shift190 = np.mean(recovery_times_shift190_minutes)\n",
    "\n",
    "print(f\"Mean recovery time for baseline: {mean_baseline} minutes\")\n",
    "print(f\"Mean recovery time for shift130: {mean_shift130} minutes\")\n",
    "print(f\"Mean recovery time for shift150: {mean_shift150} minutes\")\n",
    "print(f\"Mean recovery time for shift190: {mean_shift190} minutes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and flatten 'c_total' data for each scenario\n",
    "c_total_data = {\n",
    "    \"baseline\": result_baseline[\"metric\"],\n",
    "    \"shift130\": results_1_3[\"metric\"],\n",
    "    \"shift150\": results_1_5[\"metric\"],\n",
    "    \"shift190\": results_1_9[\"metric\"]\n",
    "}\n",
    "\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot histogram for each scenario\n",
    "for scenario, values in c_total_data.items():\n",
    "    plt.hist(values, bins=30, alpha=0.5, label=scenario)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Rob(Ψ)')\n",
    "plt.ylabel('Frequency')\n",
    "#plt.title('Histogram of c_total for Different Scenarios')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count the number of values above 300 mg/L in result_baseline[\"bod1\"]\n",
    "count_baseline_above_300 = sum(1 for value in result_baseline[\"bod1\"] if value > 300)\n",
    "# Calculate the probability\n",
    "probability_baseline_above_300 = count_baseline_above_300 / len(result_baseline[\"bod1\"])\n",
    "\n",
    "# Count the number of values above 300 mg/L in results_1_3[\"bod1\"]\n",
    "count_shift130_above_300 = sum(1 for value in results_1_3[\"bod1\"] if value > 300)\n",
    "# Calculate the probability\n",
    "probability_shift130_above_300 = count_shift130_above_300 / len(results_1_3[\"bod1\"])\n",
    "\n",
    "# Count the number of values above 300 mg/L in results_1_5[\"bod1\"]\n",
    "count_shift150_above_300 = sum(1 for value in results_1_5[\"bod1\"] if value > 300)\n",
    "# Calculate the probability\n",
    "probability_shift150_above_300 = count_shift150_above_300 / len(results_1_5[\"bod1\"])\n",
    "\n",
    "# Count the number of values above 300 mg/L in results_1_9[\"bod1\"]\n",
    "count_shift190_above_300 = sum(1 for value in results_1_9[\"bod1\"] if value > 300)\n",
    "# Calculate the probability\n",
    "probability_shift190_above_300 = count_shift190_above_300 / len(results_1_9[\"bod1\"])\n",
    "\n",
    "# Plot histogram for bodinfl_80\n",
    "plt.hist(result_baseline[\"bod1\"], bins=50, density=True, alpha=0.5, label=f'BOD infl baseline (Prob > 300 mg/L = {probability_baseline_above_300:.2f})')\n",
    "plt.hist(results_1_3[\"bod1\"], bins=50, density=True, alpha=0.5, label=f'BOD infl 130% shift (Prob > 300 mg/L = {probability_shift130_above_300:.2f})')\n",
    "plt.hist(results_1_5[\"bod1\"], bins=50, density=True, alpha=0.5, label=f'BOD infl 150% shift (Prob > 300 mg/L = {probability_shift150_above_300:.2f})')\n",
    "plt.hist(results_1_9[\"bod1\"], bins=50, density=True, alpha=0.5, label=f'BOD infl 190% shift (Prob > 300 mg/L = {probability_shift190_above_300:.2f})')\n",
    "# Add a dotted line at 300 mg/L\n",
    "plt.axvline(x=300, color='r', linestyle='--', linewidth=1, label='Threshold (300 mg/L)')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('BOD influent concentrations (mg/L)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of BOD influent concentrations for a baseline and shifts ')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of values above 50 mg/L in result_baseline[\"bod31\"]\n",
    "count_baseline_above_50 = sum(1 for value in result_baseline[\"bod31\"] if value > 50)\n",
    "# Calculate the probability\n",
    "probability_baseline_above_50 = count_baseline_above_50 / len(result_baseline[\"bod31\"])\n",
    "\n",
    "# Count the number of values above 50 mg/L in results_1_3[\"bod31\"]\n",
    "count_shift130_above_50 = sum(1 for value in results_1_3[\"bod31\"] if value > 50)\n",
    "# Calculate the probability\n",
    "probability_shift130_above_50 = count_shift130_above_50 / len(results_1_3[\"bod31\"])\n",
    "\n",
    "# Count the number of values above 50 mg/L in results_1_5[\"bod31\"]\n",
    "count_shift150_above_50 = sum(1 for value in results_1_5[\"bod31\"] if value > 50)\n",
    "# Calculate the probability\n",
    "probability_shift150_above_50 = count_shift150_above_50 / len(results_1_5[\"bod31\"])\n",
    "\n",
    "# Count the number of values above 50 mg/L in results_1_9[\"bod31\"] \n",
    "count_shift190_above_50 = sum(1 for value in results_1_9[\"bod31\"] if value > 50)\n",
    "# Calculate the probability\n",
    "probability_shift190_above_50 = count_shift190_above_50 / len(results_1_9[\"bod31\"])\n",
    "\n",
    "# Plot histogram for bodeffl31\n",
    "plt.hist(result_baseline[\"bod31\"], bins=50, density=True, alpha=0.5, label=f'BOD effl baseline (Prob > 50 mg/L = {probability_baseline_above_50:.2f})')\n",
    "plt.hist(results_1_3[\"bod31\"], bins=50, density=True, alpha=0.5, label=f'BOD effl 130% shift (Prob > 50 mg/L = {probability_shift130_above_50:.2f})')\n",
    "plt.hist(results_1_5[\"bod31\"], bins=50, density=True, alpha=0.5, label=f'BOD effl 150% shift (Prob > 50 mg/L = {probability_shift150_above_50:.2f})')\n",
    "plt.hist(results_1_9[\"bod31\"], bins=50, density=True, alpha=0.5, label=f'BOD effl 190% shift (Prob > 50 mg/L = {probability_shift190_above_50:.2f})')\n",
    "\n",
    "# Add a dotted line at 50 mg/L\n",
    "plt.axvline(x=50, color='r', linestyle='--', linewidth=1, label='Threshold (50 mg/L)')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('BOD effluent concentrations (mg/L)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of BOD effluent concentrations for a baseline and shifts')\n",
    "# Set the x-axis limits\n",
    "plt.xlim(0, 290)\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of values above 500 mg/L in result_baseline[\"cod1\"]\n",
    "count_baseline_above_500 = sum(1 for value in result_baseline[\"cod1\"] if value > 500)\n",
    "# Calculate the probability\n",
    "probability_baseline_above_500 = count_baseline_above_500 / len(result_baseline[\"cod1\"])\n",
    "\n",
    "# Count the number of values above 500 mg/L in results_1_3[\"cod1\"]\n",
    "count_shift130_above_500 = sum(1 for value in results_1_3[\"cod1\"] if value > 500)\n",
    "# Calculate the probability\n",
    "probability_shift130_above_500 = count_shift130_above_500 / len(results_1_3[\"cod1\"])\n",
    "\n",
    "# Count the number of values above 500 mg/L in results_1_5[\"cod1\"]\n",
    "count_shift150_above_500 = sum(1 for value in results_1_5[\"cod1\"] if value > 500)\n",
    "# Calculate the probability\n",
    "probability_shift150_above_500 = count_shift150_above_500 / len(results_1_5[\"cod1\"])\n",
    "\n",
    "# Count the number of values above 500 mg/L in results_1_5[\"cod1\"]\n",
    "count_shift190_above_500 = sum(1 for value in results_1_9[\"cod1\"] if value > 500)\n",
    "# Calculate the probability\n",
    "probability_shift190_above_500 = count_shift190_above_500 / len(results_1_9[\"cod1\"])\n",
    "\n",
    "# Plot histogram for codinfl\n",
    "plt.hist(result_baseline[\"cod1\"], bins=50, density=True, alpha=0.5, label=f'COD infl baseline (Prob > 500 mg/L = {probability_baseline_above_500:.2f})')\n",
    "plt.hist(results_1_3[\"cod1\"], bins=50, density=True, alpha=0.5, label=f'COD infl 130% shift (Prob > 500 mg/L = {probability_shift130_above_500:.2f})')\n",
    "plt.hist(results_1_5[\"cod1\"], bins=50, density=True, alpha=0.5, label=f'COD infl 150% shift (Prob > 500 mg/L = {probability_shift150_above_500:.2f})')\n",
    "plt.hist(results_1_9[\"cod1\"], bins=50, density=True, alpha=0.5, label=f'COD infl 190% shift (Prob > 500 mg/L = {probability_shift190_above_500:.2f})')\n",
    "# Add a dotted line at 500 mg/L\n",
    "plt.axvline(x=500, color='r', linestyle='--', linewidth=1, label='Threshold (500 mg/L)')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('COD influent concentrations (mg/L)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of COD influent concentrations for a baseline and shifts')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline\n",
    "count_baseline_above_250 = sum(1 for value in result_baseline[\"cod31\"] if value > 250)\n",
    "# Calculate the probability\n",
    "probability_baseline_above_250 = count_baseline_above_250 / len(result_baseline[\"cod31\"])\n",
    "\n",
    "#130\n",
    "count_shift130_above_250 = sum(1 for value in results_1_3[\"cod31\"] if value > 250)\n",
    "# Calculate the probability\n",
    "probability_shift130_above_250 = count_shift130_above_250 / len(results_1_3[\"cod31\"])\n",
    "\n",
    "#150\n",
    "count_shift150_above_250 = sum(1 for value in results_1_5[\"cod31\"] if value > 250)\n",
    "# Calculate the probability\n",
    "probability_shift150_above_250 = count_shift150_above_250 / len(results_1_5[\"cod31\"])\n",
    "\n",
    "#190\n",
    "count_shift190_above_250 = sum(1 for value in results_1_9[\"cod31\"] if value > 250)\n",
    "# Calculate the probability\n",
    "probability_shift190_above_250 = count_shift190_above_250 / len(results_1_9[\"cod31\"])\n",
    "\n",
    "# Plot histogram for codeffl31\n",
    "plt.hist(result_baseline[\"cod31\"], bins=50, density=True, alpha=0.5, label=f'COD effl baseline (Prob > 250 mg/L = {probability_baseline_above_250:.2f})')\n",
    "plt.hist(results_1_3[\"cod31\"], bins=50, density=True, alpha=0.5, label=f'COD effl 130% shift (Prob > 250 mg/L = {probability_shift130_above_250:.2f})')\n",
    "plt.hist(results_1_5[\"cod31\"], bins=50, density=True, alpha=0.5, label=f'COD effl 150% shift (Prob > 250 mg/L = {probability_shift150_above_250:.2f})')\n",
    "plt.hist(results_1_9[\"cod31\"], bins=50, density=True, alpha=0.5, label=f'COD effl 190% shift (Prob > 250 mg/L = {probability_shift190_above_250:.2f})')\n",
    "\n",
    "# Add a dotted line at 250 mg/L\n",
    "plt.axvline(x=250, color='r', linestyle='--', linewidth=1, label='Threshold (250 mg/L)')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('COD effluent concentrations (mg/L)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of COD effluent concentrations for a baseline and shifts')\n",
    "# Set the x-axis limits\n",
    "plt.xlim(0, 300)\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the line plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results_1_9.index, results_1_9['cod1'], label='COD1', marker='o')\n",
    "plt.plot(results_1_9.index, results_1_9['bod1'], label='BOD1', marker='x')\n",
    "\n",
    "# Adding titles and labels\n",
    "\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Concentrations BOD and COD influent')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
